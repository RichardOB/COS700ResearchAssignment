
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}

\usepackage{url}
\urldef{\mailsa}\path|richard.obrien.sa@gmail.com|  

\usepackage{cite}
   

\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}


\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Predicting Gradient Descent Failure of Feed Forward Neural Networks}



% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{Richard O'Brien \\ 10688607 \\
Dr. Katerine Malan\and Anna Rakitianskaia}


% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{University of Pretoria, Computer Science Department,\\
COS 700 Research Proposal\\
\mailsa\\
\mailsb\\
\mailsc\\
\url{http://www.cs.up.ac.za}}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

\date{March 2015}
\newpage

\title{Predicting Backpropagation Failure of Feed Forward Neural Networks}
% a short form should be given in case it is too long for the running head
\titlerunning{Predicting Backpropagation Failue of Feed Forward Neural Networks}
\author{}
\institute{}

\authorrunning{10686607}

\maketitle

\begin{abstract}
The abstract should summarize the contents of the paper and should
contain at least 70 and at most 150 words. It should be written using the
\emph{abstract} environment. Content includes 1) a brief background to the research (no citations) and
a problem statement, 2) Method(s) used, 3) Expected Results. This is usually easier to write last and
should not be just a cut and paste from the proposal - write it in a more condensed form.

\keywords{Performance Prediction, Neural Network, Backpropagation, Error Landscape Analysis}
\end{abstract}


\section{Introduction}

Backpropagation is a powerful tool that is widely used in many diverse fields ranging from neural networks and fuzzy logic structures to 
economic structures. Backpropagation is amongst the most popular methods for training artificial neural networks \cite{chauvin1995backpropagation, rumelhart1985learning, rumelhart1988learning} 
and has seen applications in pattern recognition, sensitivity analysis and dynamic modeling \cite{chauvin1995backpropagation}.
\\\\
However, backpropagation has been known to fail in some situations by reaching sub-optimal results \cite{brady1989back, sexton1998toward} and has even seen negative 
comparisons against alternative training algorithms such as particle swarm optimization \cite{gudise2003comparison} and genetic algorithms \cite{sexton1998toward}. 
\\\\
Although a general understanding of the reasons behind backpropagation failure is known \cite{gori1992problem}, a deeper analysis is desired at a much lower level in order to gain more insight
into problem features where backpropagation performs badly. Many studies have analysed the relationship between problem characteristics and algorithm performance 
\cite{smith2008towards, bischl2012algorithm}, leading to successfull algorithm performance predicion \cite{munoz2012meta, malan2014particle}. However, no study has analysed such a relationship 
between problem characteristics and backpropagation performance. 
\\\\
It is thus desired to investigate possible relationships between problem characteristics and backpropagation performcance, in order to establish a failure prediction model. Establishment of a successful
prediction model will help one know in advance if backpropagation will fail on a given problem. 

\section{Problem Statement}

Due to a lack of understanding of the link between low level problem characteristics and backpropagation performance, 
it is not known in advance if the use of backpropagation to train feedforward neural networks will fail to solve a particular problem. 
This leads to potentially avoidable time costs.
Analysing the relationship between various problem characteristics and the success classes of backpropagation applied on such problems 
could lead to the establishment of a prediction model. With such a model, the performance of backpropagation on a particular problem can be 
evaluated in advance at a fraction of the cost.
\\\\
This work does not aim to solve the algorithm selection problem for the training of neural networks but instead will help to establish new knowledge 
regarding problem difficulty and backpropagation.
\\\\
Objectives:
\begin{enumerate}
 \item To characterise the landscape features of a selection of problems.
 \begin{enumerate}
  \item Establish a set of benchmark problems upon which backpropagation is known to struggle as well as problems where it is known to succeed.
  \item Determine a chosen failure threshold for each problem. 
  \item Evaluate the performance of backpropagation on each of the problems by running each problem through the feed forward neural network.
  \item Determine problem characteristics by analysing the fitness landscape of each problem using exploratory landscape analysis.
  \item Investigate the relationship between backpropagation performance and problem characteristics.
 \end{enumerate}
 
  \item To analyse the robustness of the error landscape analysis (ELA) measures for neural network error surfaces.
 \begin{enumerate}
  \item Analyse the effect on ELA measures after changes to the search space boundaries (range of weight values).
  \item Analyse the effect of ELA measure parameters in the context of neural networks.
 \end{enumerate}
 
 \item To establish a failure prediction model for backpropagation.
 \begin{enumerate}
  \item Establish a dataset for data mining.
  \item Perform decision tree induction using data mining tools.
  \item Investigate the performance of decision tree models created with different parameters.
  \item Establish and analyse automatic failure prediction using a neural network.
  \item Investigate the performance of a lazy classifier such as nearest neighbor. 
 \end{enumerate}
 
\end{enumerate}

\section{Methodology}

The following section describes the methodologies that will be used during this case study. Error landscape analysis will be applied on neural networks, analysing the 
insight it may provide on the failure prediction of backpropagation. This will require many objectives to be met. 
\\\\
In order to characterise the landscape features of a selection of problems, an initial set of benchmark problems will need to be collected. To obtain a true representation 
of the performance of backpropagation, the initial set of benchmark problems will be required to contain problems upon which back propagation is known to fail, as well as problems where it 
is known to succeed. A collection of classification and regression problems will need to be obtained, and if necessary, prepared for neural network training. The preparation 
of the dataset will ensure that data is not distorted or missing, influencing the overall performance of the neural network. The dataset for each problem will then need
to be split into training and test data. An argument will then need to be made regarding each chosen failure threshold, determined based on a review of the literature.
\\\\
A feed forward neural network using backpropagation training will need to be written in a chosen high level language. The feed forward neural network will be tested thoroughly 
to ensure the correct behaviour of the algorithm. The already established failure threshold will be used to determine the performance of the neural network on each problem after
all of the unseen data has passed through the trained neural network. Results from the neural network will need to be written to a file in a format which can be 
analysed and interpreted correctly.
\\\\
Exploratory landscape analysis functions, previously written in R will be used to analyse the fitness landscape of each problem. Relevant functions will need to be determined
and selected. 
\\\\
In order to analyse the robustness of the ELA measures for neural network error surfaces, experiments will need to be conducted in which results will be recorded after various changes
to the search space boundaries as well as various changes to ELA parameters. The recorded results will then need to be compared in order to determine 
if the measures are robust when applied to neural network error surfaces.
\\\\
A collection of machine learning algorithms for data mining tasks such as Weka will be used to establish, evaluate and analyse decision tree models for failure prediction.
A neural network will be used for automatic failure prediction, potentially creating a more accurate prediction model. A lazy classifier such as nearest neighbor will also be
used in an attempt to predict failure. 

\section{Literature Survey}

Gradient Descent, a popular optimisation algorithm, forms the basis of Error Backpropagation which is 
the most widely used training algorithm for Multilayer Feedforward Neural Networks (FFNNs). 
When solving classification and function approximation problems, FFNNs are the preferred neural network architectures 
due to their learning and generalization abilities \cite{gong2012training}. Gradient Descent is even used in oil and gas pipelines, 
training Traditional Back Propagation Neural Networks which are responsible for the condition recognition of pipelines \cite{laibin2009novel}.

Despite it's popularity, there exists complexities and limitations intrinsic to Gradient Descent. Gradient Descent is known to fail in
several different ways, including but not limited to the algorithm reaching a poor local minima, reaching an unsatisfactory global minima,
encountering numerical precision problems, experiencing slow convergence time, and getting stuck in long plateaus \cite{baldi1995gradient,cetin1993global,laibin2009novel,soni2013performance}

However, the reasons for the above limitations are not independant or well understoond \cite{baldi1995gradient}. Knowing in advance, where Gradient 
Descent is going to fail will help to avoid uneccessary function costs. In order to gain a deeper understanding of what situations Gradient Descent is likely to fail
will require an anlaysis of low level problem characteristics.
\\\\
Fitness Landscape's are a representation of the search space with regards to
fitness values\cite{merkuryeva2010comparative}. ``A fitness landscape is obtained by associating a fitness with each point in the search space''\cite{uludag2009fitness} upon which analysis can be performed to derive
the behaviour and characteristics of a particular algorithm. Error Landscapes follow a similar approach but instead associate an error value with each point in the search space.
\\
Fitness Landscapes have most recently been used to measure multiple characteristics of a particular problem. Amongst the many studies on measuring problem characteristics, Fitness Landscapes have been used to characterize
and distinguish instances of circle packing problems \cite{morgan2014fitness} as well as the characterization of two dimensional numerical optimization problems \cite{munoz2012landscape}. Fitness Landscape Characteristics 
have been applied to Algorithm Performance Prediction \cite{smith2008towards,malan2014particle,mersmann2013novel} and the Algorithm Selection Problem (ASP) \cite{bischl2012algorithm}. Our study will involve generating an Error Landscape of
a FFNN using Gradient Descent for various optimisation and regression problems. The Error Landscape will then be studied in order to extract unique problem characteristics which can be compared with algorithm performance.
\\
There exists many Fitness Landscape Analysis techniques which can be used to analyse the characteristics of a landscape. Techniques which require a simple random sample of solution points include Fitness Distance Correlation \cite{gallagher2001fitness}, 
Exploratory Landscape Analysis \cite{mersmann2011exploratory}, Density of States \cite{rose1996density} and many others \cite{malan2013survey}. For the purpose of this study, Exploratory Landscape Analysis 
will be used due to the techniques use of ``relatively cheap low level computer generated features'' \cite{mersmann2011exploratory}.
\\\\
Exploratory Landscape Analysis involves an ``approach to automatically extract low-level features from systematically sampled points in the decision space'' \cite{mersmann2011exploratory}. These low-level features provide insight into the characteristics
of the problem. Exploratory Landscape Analysis has become popular due to the low cost of Error Landscape feature extraction. Exploratory Landscape Analysis has been used to analyse problem features of the Travelling Salesperson Problem, resulting in
the identification of important features responsible for appproximation difficulty by 2-opt based local search strategies \cite{mersmann2013novel}. Although Exploratory Landscape Analysis has been used to introduce a novel solution to the 
Algorithm Selection Problem based on Cost-Sensitive Learning \cite{bischl2012algorithm}, there have been no studies applying Exploratory Landscape Analysis on Neural Network Error Landscapes. This study intends to use Exploratory Landscape Analysis to extract 
features of a variety of problems which can be used to form a dataset with gradient descent failure indicators. Through Data Mining techniques, this dataset could be used to establish a Gradient Descent Failure Prediction Model. 
\\\\
Algorithm Failure Prediction involves finding a link between algorithm performance and problem characteristics. Once a link is found, a failure prediction model can be established, allowing one to know in advace whether or not to apply a particular
algorithm to the problem at hand. Promising results have been seen from recent research. Malan \textit{et al.} \cite{malan2014particle} used Fitness Landscape metrics on a range of benchmark functions to induce decision tree models for seven different 
PSO algorithms. The prediction models showed high levels of accuracy for predicting failure, and offered new insight into the algorithms themselves. Smith-Miles \cite{smith2008towards} used meta-learning concepts to find a relationship between search space 
characteristics and algorithm performance. A Neural Network was then used to successfully predict the performance of three metaheuristics. Bischl \textit{et al.} \cite{bischl2012algorithm} used low-level features, obtained through Exploratory Landscape Analysis, to predict which algorithms
within a portfolio would perform well for each function. Despite recent success in this area, Failure Prediction has yet to be applied to Gradient Descent using Error Landscape Characteristics. 
This study will attempt to induce Failure Predictions models using Decision Tree Induction and Neural Networks, based on problem characteristcis found through Exploratory Landscape Analysis.
\\\\
Possible Concerns regarding Exploratory Landscape Analysis measures have recently been raised. Munoz \textit{et al.} \cite{munoz2015effects} provides a cautionary examination of the use of ELA measures in black-box continuous optimisation, exploring the effect of translations, and the robustness of the 
neighborhood structure after performing dimensionality reductions.\\After prediction models have been successfully established, this study will examine the robustness of the ELA measures after boundary constrained weight region adjustments and dimensionality reduction.

\section{Project Planning}
 
\begin{enumerate}

 \item Identify and describe the specific phases of your work
 \begin{enumerate}
  \item Such as reading, writing, coding, experimentation, testing, writing, editing, writing.
 \end{enumerate}
 
 \item Provide a schedule of everything that needs to be done all the way up to the submission date (5 November) or Presentation Date
 \begin{enumerate}
  \item Tasks that need to be done and the time allocated to them with deadlines.
  \item Remember that your supervisor needs time to review your report/paper and you need time to incorporate changes before the submission date.
 \end{enumerate}
 
 \item Advantages of Task Timelines:
 \begin{enumerate}
  \item Forces you to break down the work into manageable chunks.
  \item Lets you think about the logical sequence of tasks.
  \item Planning helps you to not miss out important tasks.
  \item Creates deadlines for yourself.
  \item Clears your head
  \item Can be used to map progress.
 \end{enumerate}
 
 \item Some Hints:
 \begin{enumerate}
  \item Be realistic, rather pad the time than set unrealistic deadlines.
  \item Don't forget to factor in other things (other modules, your life, ...)
  \item Some tasks can run concurrently - good to have alternative tasks to work on.
  \item Give page estimates to writing tasks.
  \item Start from end and plan backwards.
  \item Commit to your plan: have your timeline visible somewhere
  \item Update your plan if necessary as you progress through the work.
 \end{enumerate}

\end{enumerate}

\bibliographystyle{ieeetr}
\bibliography{bibfile}

\end{document}
